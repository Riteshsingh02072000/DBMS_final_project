package SalesCountry;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import java.io.BufferedReader;
import java.io.BufferedWriter;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.io.IOException;
import java.util.*;


public class SalesCountryDriver {


	 private static List<String[]> readDataFromFile(String inputFile) throws IOException {
        List<String[]> data = new ArrayList<>();

        try (BufferedReader reader = new BufferedReader(new FileReader(inputFile))) {
            String line;
            while ((line = reader.readLine()) != null) {
                String[] columns = line.split("\t");
                data.add(columns);
            }
        }

        return data;
    }

    private static void writeDataToFile(List<String[]> data, String outputFile) throws IOException {
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(outputFile))) {
            for (String[] columns : data) {
                writer.write(String.join("\t", columns));
                writer.newLine();
            }
        }
    }
	

	public static void main(String[] args) {
		JobClient my_client = new JobClient();
		// Create a configuration object for the job
		JobConf job_conf = new JobConf(SalesCountryDriver.class);

		// Set a name of the Job
		job_conf.setJobName("SalePerCountry");

		// Specify data type of output key and value
		job_conf.setOutputKeyClass(Text.class);
		job_conf.setOutputValueClass(DoubleWritable.class);

		// Specify names of Mapper and Reducer Class
		job_conf.setMapperClass(SalesCountry.SalesMapper.class);
		job_conf.setReducerClass(SalesCountry.SalesCountryReducer.class);

		// Specify formats of the data type of Input and output
		job_conf.setInputFormat(TextInputFormat.class);
		job_conf.setOutputFormat(TextOutputFormat.class);

		// Set input and output directories using command line arguments, 
		//arg[0] = name of input directory on HDFS, and arg[1] =  name of output directory to be created to store the output file.
		
		FileInputFormat.setInputPaths(job_conf, new Path(args[0]));
		FileInputFormat.setInputPaths(job_conf, new Path(args[1]));
		FileOutputFormat.setOutputPath(job_conf, new Path(args[2]));
		

		my_client.setConf(job_conf);
		try {
			// Run the job 
			JobClient.runJob(job_conf);
		}
		catch(Exception e)
		{

		}
        Configuration conf = new Configuration();
        conf.set("fs.defaultFS", "hdfs://localhost:9820");

        // try {
        //     // Create Hadoop FileSystem object
        //     FileSystem fs = FileSystem.get(conf);

        //     // Specify the path of the output file generated by MapReduce
        //     Path outputFilePath = new Path("hdfs://localhost:9820/"+args[2]+"/part-00000");

        //     // Sort the output file in HDFS
        //     sortFileInHdfs(fs, "hdfs://localhost:9820/"+args[2]);

        //     System.out.println("Output file sorted successfully.");
        // } catch (Exception e) {
        //     e.printStackTrace();
        // }

	}
}